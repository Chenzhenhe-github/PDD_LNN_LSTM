{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77c49ee",
   "metadata": {},
   "source": [
    "# build a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d594d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set parameter settings\n",
    "training = 'training' # Name of the training set folder\n",
    "group_num = [73,49,99] # Arranged in Windows system order\n",
    "group_name = ['H','MP','P']\n",
    "mass_rate = 0.90 # m/z detection ratio\n",
    "\n",
    "# Test set parameter settings\n",
    "tolerance = 0.5 # Mass spectrum peak resolution selection, do not change, tests show 0.5 is better than 0.3!\n",
    "peak_int_learn = 0.01 # Select mass spectrum peaks based on peak intensity\n",
    "pvalues = 0.05 # p-value screening\n",
    "\n",
    "# Dataset paths\n",
    "path = .'/'+training+'/' # file route of single mass files continging m/z and intensity\n",
    "path_store =  .'/'+training+'_store/' # file store route of training file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2d500",
   "metadata": {},
   "source": [
    "# Perform m/z alignment and filtering through pyopenms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099e8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyopenms as oms\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "# Data quality control, mass spectrum peak denoising, remove flat-top peaks\n",
    "def noise_removal(prim,tolerance): \n",
    "    total = prim.values.tolist()\n",
    "    ref_total = total[1:]+[[0,0]]\n",
    "    new_total = [[r[0]-m[0],r[1]-m[1]] for r,m in zip(ref_total,total)]   \n",
    "    tf = [total[0]]\n",
    "    for new,ref,to in zip(new_total,ref_total,total):\n",
    "        if new[0] >= 0.5:        \n",
    "            tf = tf+[ref]\n",
    "        else:\n",
    "            if new[1]>=0:                      \n",
    "                tf = tf[:-1]+[ref]+[ref]\n",
    "            else:\n",
    "                tf = tf[:-1]+[to]+[to]\n",
    "    tf = [m for i,m in enumerate(tf) if m not in tf[:i]]            \n",
    "    return tf\n",
    "\n",
    "# Generate OpenMS data format\n",
    "def openms_data_format(mass,intensity,decimal=5):\n",
    "    # Retain mass spectrum data\n",
    "    mz = np.round(mass.values,decimal)\n",
    "    mz_intensity = intensity.values\n",
    "    spectrum = oms.MSSpectrum()\n",
    "    spectrum.set_peaks([mz,mz_intensity])\n",
    "    spectrum.sortByPosition()\n",
    "    return spectrum\n",
    "\n",
    "# Mass alignment\n",
    "def mass_align(ref_spectrum,obs_spectrum,tolerance=0.5):\n",
    "    alignment = []\n",
    "    spa = oms.SpectrumAlignment()\n",
    "    p = spa.getParameters()\n",
    "    # use 0.5 Da tolerance (Note: for high-resolution data we could also use ppm \n",
    "    # by setting the is_relative_tolerance value to true)\n",
    "    p.setValue(\"tolerance\", tolerance)\n",
    "    p.setValue(\"is_relative_tolerance\", \"false\")\n",
    "    spa.setParameters(p)\n",
    "    # align both spectra\n",
    "    spa.getSpectrumAlignment(alignment, ref_spectrum, obs_spectrum)\n",
    "    return alignment\n",
    "\n",
    "# Mass calculation: average (version 1)\n",
    "def mass_calculation(re_spectrum,ob_spectrum,alignment,decimal=4):   \n",
    "    ref = [i[0] for i in alignment]\n",
    "    obs = [j[1] for j in alignment]\n",
    "    ref_mass = [re_spectrum.mass[i] for i in ref]\n",
    "    obs_mass = [ob_spectrum.mass[j] for j in obs]\n",
    "    ave_mass = np.round((np.array(ref_mass)+np.array(obs_mass))/2,decimal)\n",
    "    for i,j,q in zip(ref,obs,range(len(ave_mass))):\n",
    "        re_spectrum.iloc[i, 0] = ave_mass[q]\n",
    "        ob_spectrum.iloc[j, 0] = ave_mass[q]\n",
    "    return re_spectrum,ob_spectrum\n",
    "\n",
    "# Mass calculation based on reference file (version 2)\n",
    "def mass_calculation_ref(re_spectrum,ob_spectrum,alignment,decimal=4):   \n",
    "    ref = [i[0] for i in alignment]\n",
    "    obs = [j[1] for j in alignment]\n",
    "    for i,j in zip(ref,obs):\n",
    "        ob_spectrum.iloc[j, 0] = re_spectrum.iloc[i, 0]         \n",
    "    return re_spectrum,ob_spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4215534",
   "metadata": {},
   "source": [
    "# Generation of initial file (default to use the first data file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6616a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from jcamp import jcamp_readfile\n",
    "\n",
    "# Read file list 2023-09-22\n",
    "file_list = natsorted(os.listdir(path))\n",
    "column_list = [fst.split('.')[0] for fst in file_list]\n",
    "\n",
    "# Add initial file values 2023-09-22\n",
    "name_list = file_list[1:]\n",
    "col_list = column_list[1:]\n",
    "\n",
    "# Generate initial file 2023-09-22\n",
    "first_file,first_column = file_list[0],column_list[0]\n",
    "prim = pd.read_excel(path+first_file)\n",
    "prim = noise_removal(prim,tolerance)\n",
    "prim = pd.DataFrame(prim,columns=['mass',first_column])\n",
    "#prim = prim[prim[first_column] >= peak_int_learn*max_peak].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa08be",
   "metadata": {},
   "source": [
    "# Remaining samples addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa27b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Training set generation 20241124\n",
    "for name, col in zip(name_list, col_list):\n",
    "    # Read file\n",
    "    indata = pd.read_excel(path + name)\n",
    "    denoise = noise_removal(indata, tolerance)  # Remove noise\n",
    "    framefile = pd.DataFrame(denoise, columns=['mass', col])\n",
    "\n",
    "    # Generate openms data\n",
    "    ref_spectrum = openms_data_format(prim.mass, prim.iloc[:, 1])\n",
    "    obs_spectrum = openms_data_format(framefile.mass, framefile.iloc[:, 1])\n",
    "    alignment = mass_align(ref_spectrum, obs_spectrum, tolerance)\n",
    "\n",
    "    # Data integration\n",
    "    r_spectrum, o_spectrum = mass_calculation_ref(prim, framefile, alignment)\n",
    "    prim = pd.merge(prim, o_spectrum, how='outer', on='mass')  # Merge is not easy to use well\n",
    "    prim = prim.sort_values('mass', ascending=True).reset_index(drop=True)  # Merge is not easy to use well\n",
    "\n",
    "# Final data generation\n",
    "outfile = prim.replace(0, np.nan).dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26a777",
   "metadata": {},
   "source": [
    "# Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49371ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save training set\n",
    "prim_int = outfile\n",
    "\n",
    "# Calculate data statistics\n",
    "data_num = [0]\n",
    "for p in range(len(group_num)):\n",
    "    data_num.append(sum(group_num[:p+1]))\n",
    "\n",
    "minum_num = [round(m * mass_rate) for m in group_num]\n",
    "new_group_name = ['num_'+ name for name in group_name]\n",
    "\n",
    "# Generate new data columns\n",
    "for i,name in zip(range(len(data_num)-1),new_group_name):\n",
    "    prim_int[name] = prim_int.iloc[:,data_num[i]+1:data_num[i+1]+1].count(axis=1)\n",
    "\n",
    "# File filtering\n",
    "total_file = prim_int[(prim_int[new_group_name[0]] >= minum_num[0])]    \n",
    "for name,mini in zip(new_group_name[1:],minum_num[1:]):\n",
    "    internal_file = prim_int[(prim_int[name] >= mini)]  \n",
    "    total_file = pd.merge(total_file,internal_file,how='outer') \n",
    "int_scd = total_file.copy().sort_values(by = 'mass').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30505dcb",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "#SimpleImputer, NaN data imputation, optional\n",
    "#strategy=['mean','median','most_frequent']\n",
    "#Data Regularization\n",
    "The sum of the absolute values of the 'l1' eigenvalues, the square root of the sum of the absolute values squared of the 'l2' eigenvalues, and the maximum value of the 'max' eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9848e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "#-----------------Dataset processing------------------\n",
    "makepip = make_pipeline(SimpleImputer(missing_values=np.nan,\\\n",
    "                                      strategy='median'),Normalizer(norm='l1'),StandardScaler())\n",
    "int_sc = int_scd[column_list].T\n",
    "treat = makepip.fit(int_sc)\n",
    "int_sc_norm = pd.DataFrame(treat.transform(int_sc).T,columns=column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05abd691",
   "metadata": {},
   "source": [
    "# Statistical analysis and data storage\n",
    "#Data Statistical Analysis No.2: Inter-group Comparison and Statistics #Revised on September 16th, using np.nanmean() to calculate the average #Revised on February 19th, 2025, adding ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37654efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save training set\n",
    "from scipy import stats\n",
    "# Split the dataset\n",
    "internal_list = []\n",
    "for i,name in zip(range(len(data_num)-1),new_group_name):\n",
    "    data_internal = int_sc_norm.iloc[:,data_num[i]:data_num[i+1]].T.values\n",
    "    internal_list.append(data_internal)\n",
    "\n",
    "# Determine whether ANOVA analysis is needed\n",
    "if len(group_num)>=3:\n",
    "    f_statistic, p_value = stats.f_oneway(*internal_list)\n",
    "    p_column = 'ANOVA'\n",
    "else:\n",
    "    s_statistic, p_value = stats.ttest_ind(*internal_list,equal_var=False)\n",
    "    p_column = 'ttest'\n",
    "    \n",
    "# Filter m/z with p-value less than threshold\n",
    "p_value = pd.DataFrame(p_value,columns = [p_column])\n",
    "f_value = pd.DataFrame(f_statistic,columns = [p_column+'_f'])\n",
    "int_scf = pd.concat([int_scd,p_value,f_value],axis=1)\n",
    "int_F = int_scf[(int_scf[p_column] <= 0.05)]\n",
    "int_F_sort = int_F.sort_values(by = ['mass'])\n",
    "\n",
    "# Save data\n",
    "int_F_sort.to_excel(path_store+training+'_screened_int.xlsx',index=False)\n",
    "int_scf.to_excel(path_store+training+'_int.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
